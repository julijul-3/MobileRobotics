{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1ca6fb-8cf7-43c5-b56d-c7a22c33d47d",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    \n",
    "<li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\" Introduction\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#2.-Environment-of-the-project\" data-toc-modified-id=\"2.Environment of the project\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment of the project</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#3.-Method-overview\" data-toc-modified-id=\"3.-Method-overview\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Method overview</a></span></li>\n",
    "\n",
    "<li><span><a href=\"#4.-Computer-vision\" data-toc-modified-id=\"4.-Computer-vision\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Computer vision</a></span>\n",
    "    <ul class=\"toc-item\"><li><span><a href=\"#4.1.-Pre-processing-the-raw-image:\" data-toc-modified-id=\"4.1.-Pre-processing-the-raw-image:\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Pre-processing the raw image:</a></span></li>\n",
    "    <li><span><a href=\"#4.2.-Identification-of-Robot-State:\" data-toc-modified-id=\"4.2.-Identification-of-Robot-State:\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Identification of Robot State:</a></span></li>\n",
    "    <li><span><a href=\"#4.3.-Grid-Analysis:\" data-toc-modified-id=\"4.3.-Grid-Analysis:\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Grid Analysis:</a></span></li></ul></li>\n",
    "\n",
    "<li><span><a href=\"#5.-Motor-Control\" data-toc-modified-id=\"# 5.Motor-Control\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Motor Control</a></span></li>\n",
    "\n",
    "    \n",
    "<li><span><a href=\"#6.-Filtering\" data-toc-modified-id=\"#6.-Filtering\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Filtering</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#7.-Global-Localisation\" data-toc-modified-id=\"#7.-Global-Localisation\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Global Localisation</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#8.-Local-Navigation\" data-toc-modified-id=\"#8.-Local-Navigation\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Local Navigation</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#9.-Conclusion\" data-toc-modified-id=\"#9.!Conclusion\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li>\n",
    "    \n",
    " <li><span><a href=\"#10.-References\" data-toc-modified-id=\"#10.-References\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>References</a></span></li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6144eb-6f93-4be8-a46d-30fdc4bbac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2aa168-f8fb-4522-8e1d-83dbfe0bb05a",
   "metadata": {},
   "source": [
    "# Vision guided navigation with Thymio - Final project\n",
    "Authors: Ahmed Boubakry, Charles de Fournas, Julie Favre, Paul Richard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0ce4f-e8f5-469b-84f1-67185ed29d7c",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826cb1f-c8f4-4695-8133-2f97425821ee",
   "metadata": {},
   "source": [
    "The objective of this project is to develop a comprehensive robotic system for the Thymio robot, enabling it to navigate efficiently on a two-dimensional plane. The primary focus involves implementing advanced algorithms and methods to achieve global and local navigation, obstacle avoidance, accurate position estimation, and trajectory correction. The key components of our project include:\n",
    "\n",
    "- Computer Vision in order to create the map and retrieve the robot's position: Utilizing computer vision techniques, we aim to create a map of the environment and accurately determine the Thymio robot's position in real-time<br><br>\n",
    "\n",
    "\n",
    "- A* algorithm to find the optimal path:,Implementing the A* algorithm to find the optimal path from the starting position to the specified goal, considering the map and any obstacles encountered<br><br>\n",
    "\n",
    "- Local navigation to avoid physical objects: Developing local navigation methods to enable the Thymio robot to dynamically adjust its path in real-time, effectively avoiding physical obstacles encountered during its movement<br><br>\n",
    "\n",
    "\n",
    "- Filtering with Extended Kalman Filter to estimate accuretly the position: Employing the Extended Kalman Filter for precise and accurate estimation of the robot's position, enhancing the overall reliability of the navigation system and still providing the position of the robot if we don't see the camera<br><br>\n",
    "\n",
    "- Controller to correct the trajectory after every move: The robot will change his direction in accord with the camera and theorical position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2448ca-696c-49ab-b2e7-a7e5865c131a",
   "metadata": {},
   "source": [
    "## 2. Environment of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344aa7ed-e773-43e4-b3b1-13c8021e2ba8",
   "metadata": {},
   "source": [
    "We have opted for a black and white grid as our environment, where white cells signify walkable areas and black cells represent walls. The robot's objective is to determine the optimal path from its starting position to a designated goal cell, indicated in blue.\n",
    "\n",
    "For visualization purposes, we print the grid on an A0 page with 8 columns and 6 rows. The page's borders serve as reference points for the camera to deduce the grid. We've outlined the grid using black tape, aiding in its detection.\n",
    "\n",
    "Obstacles are depicted by black cartoon representations, and the goal is visually distinguished by a blue cartoon. Additionally, on the robot, we've strategically positioned two black points – a smaller one in front of a larger one – to help the camera ascertain the robot's orientation.\n",
    "\n",
    "The camera is mounted on a tripod with an extension, positioning it vertically above the center of our environment.\n",
    "\n",
    "To enhance the camera's visibility, we've strategically placed multiple lamps to illuminate the area. These lamps are intended to provide ample lighting for improved camera performance.\n",
    "\n",
    "In consideration of the Thymio sensor, obstacles are designed to be solid, ensuring effective detection by the sensor. It's important to note that these obstacles are shaped differently from rounds, as our camera primarily detects round forms for our robot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be0124-9229-4164-8a21-a4eeddaed404",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"image/env_2.jpg\" alt=\"environnement\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d9882-a9d1-42ad-a400-0d060bb87175",
   "metadata": {},
   "source": [
    "## 3. Method overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184673e-8117-4314-8d59-497bfee9b43b",
   "metadata": {},
   "source": [
    "The program begins with the initialization of global variables and the importation of modules. Initially, we extract the grid and the robot position. The subsequent significant part of our code focuses on finding the optimal path to reach the destination.\n",
    "\n",
    "Once the setup is complete, the main loop initiates. The robot moves from one position to another, and at regular intervals, the program analyzes and corrects its position. During each iteration, the loop checks for potential obstacles. If an obstacle is detected, a corresponding function is triggered to navigate around it. Otherwise, the program proceeds as usual, initiating a new loop to progress to the next step.\n",
    "\n",
    "This loop continues until the final destination is reached. The structure ensures that the robot dynamically adapts its path, corrects its trajectory, and navigates around obstacles, providing a robust and efficient navigation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc0698-0dc7-4dcb-9e75-542b15c14bc1",
   "metadata": {},
   "source": [
    "<img src=\"image/logic_map.jpg\" alt=\"Method overview\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70684718-1daa-4551-8393-57b13538ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node a13184be-b7ac-45c1-84e9-c4292b7f7ff2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import asyncio\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ExtendedKalmanFilter import EKF\n",
    "import IPython.display as Disp\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a39869-e8b3-428d-b77f-5e3cc12ac985",
   "metadata": {},
   "source": [
    "#### Global Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af16fb5-be22-4477-946b-457d8fb21494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- global initialization -----------------\n",
    "x_end = y_end=0\n",
    "\n",
    "class RobotState:\n",
    "    def __init__(self, x=0, y=0, r=0, angle=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.angle = angle \n",
    "\n",
    "robot_state = RobotState()\n",
    "\n",
    "\n",
    "# size of our environment grid\n",
    "num_cases_x = 8  # horizontal cells (valeur arbitraire ici juste pour que ca marche dans mon exemple) \n",
    "num_cases_y = 6   # vertical cells\n",
    "\n",
    "# Initializing our matrix containing the environment information\n",
    "center_matrix = [[None for _ in range(num_cases_x)] for _ in range(num_cases_y)]\n",
    "state_matrix = [[None for _ in range(num_cases_x)] for _ in range(num_cases_y)]\n",
    "debug_matrix = [[None for _ in range(num_cases_x)] for _ in range(num_cases_y)]\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#--------------- CV initialization -------------------\n",
    "\n",
    "#camera related initialization \n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "#Lower the resolution\n",
    "cam.set(3, 1280)\n",
    "cam.set(4, 720)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#--------------- Kalman initialization ---------------\n",
    "ekf = None\n",
    "last_time = time.monotonic()\n",
    "#-----------------------------------------------------\n",
    "\n",
    "#------------- Global Nav initialization -------------\n",
    "count = 0\n",
    "obstacle_state = 1\n",
    "was_kidnapped = False\n",
    "new_angle = 0\n",
    "#-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cdb9f-f2bd-40af-8925-242ebe4fd19a",
   "metadata": {},
   "source": [
    "#### Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b628c1-7473-4df1-a263-e5a118495541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle_degrees(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculate the angle in degrees between two points (x1, y1) and (x2, y2).\n",
    "    \"\"\"\n",
    "    delta_x = x2 - x1\n",
    "    delta_y = y2 - y1\n",
    "    angle_rad = math.atan2(delta_y, delta_x)\n",
    "    \n",
    "    # Convert angle to degrees\n",
    "    angle_degrees = math.degrees(angle_rad)\n",
    "\n",
    "    return angle_degrees\n",
    "\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two points (x1, y1) and (x2, y2).\n",
    "    \"\"\"\n",
    "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return distance\n",
    "\n",
    "def truncate_coordinate(center_matrix, X, Y):\n",
    "    '''\n",
    "    Converts pixel coordinates into map coordinates. \n",
    "    It searches through a matrix to find the closest point to the given X, Y coordinates, \n",
    "    considering a certain threshold. Returns the corresponding map coordinates.\n",
    "\n",
    "    :param center_matrix: The matrix representing the map.\n",
    "    :param X: The X coordinate in pixels.\n",
    "    :param Y: The Y coordinate in pixels.\n",
    "    :return: The corresponding (x, y) coordinates on the map.\n",
    "    '''\n",
    "    threshold_x = 60\n",
    "    threshold_y = 60\n",
    "    num_cases_y, num_cases_x = center_matrix.shape[:2]  # Get the dimensions of the matrix\n",
    "\n",
    "    for i in range(num_cases_y):\n",
    "        for j in range(num_cases_x):\n",
    "            diff_x = abs(center_matrix[i, j][0] - X)\n",
    "            diff_y = abs(center_matrix[i, j][1] - Y)\n",
    "\n",
    "            if diff_x <= threshold_x and diff_y <= threshold_y:\n",
    "                x_out, y_out = j, i  \n",
    "    return(x_out, y_out)\n",
    "\n",
    "def symetrie_lignes(matrix):\n",
    "    '''\n",
    "    Performs a line symmetry transformation on the given matrix.\n",
    "    This function inverts the rows of the matrix, swapping the first row with the last,\n",
    "    the second row with the second to last, and so on.\n",
    "\n",
    "    :param matrix: The matrix to be transformed.\n",
    "    :return: The transformed matrix with inverted rows.\n",
    "    '''\n",
    "    copy_matrix = copy.deepcopy(matrix)\n",
    "    matrix[0][:] = copy_matrix[5][:]\n",
    "    matrix[1][:] = copy_matrix[4][:]\n",
    "    matrix[2][:] = copy_matrix[3][:]\n",
    "    matrix[3][:] = copy_matrix[2][:]\n",
    "    matrix[4][:] = copy_matrix[1][:]\n",
    "    matrix[5][:] = copy_matrix[0][:]\n",
    "    return matrix\n",
    "    \n",
    "def get_speed(right, left):\n",
    "    \"\"\"Retrieves the speed of the robot. Constant coefficient to converse speed of motors to units we want\n",
    "    returns: np.array- [linear_speed, angular speed] in pixels/sec and angle/sec\n",
    "    \"\"\"\n",
    "    coeff_lin = 176/7\n",
    "    coeff_angular = 37/200\n",
    "    linear = ((right + left)/2) * coeff_lin\n",
    "    angular = (right - left) * coeff_angular\n",
    "    return np.array([linear, angular])\n",
    "\n",
    "def normalize_angle(angle):\n",
    "    \"\"\"Normalize an angle to the range [-180, 180] degrees.\"\"\"\n",
    "    angle = angle % 360  # Normalize between 0 and 360\n",
    "    if angle > 180:\n",
    "        angle -= 360  # Adjust to the range [-180, 180]\n",
    "    return angle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea1f36-aacd-4530-8a1c-d475c7e61ec4",
   "metadata": {},
   "source": [
    "## 4. Computer vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717716b-7b53-412c-b2ab-fb30c4820b07",
   "metadata": {},
   "source": [
    "The integration of computer vision is crutial in our project for initializing and periodically updating the data points necessary for autonomous navigation. This encompasses the robot's current location, the orientation, the goal's coordinates, and the matrices that detail the state and the centers of the grid cells. To fulfill these requirements, we employ the info_robot_cam() function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e64939-0862-4f78-8a7c-ac5178492bc6",
   "metadata": {},
   "source": [
    "```python\n",
    "def info_robot_cam():\n",
    "    global center_matrix, state_matrix\n",
    "    image_not_good = True\n",
    "    while image_not_good: \n",
    "        image_not_good = False\n",
    "        ret, frame = cam.read()\n",
    "        if ret:\n",
    "            result_image = frame.copy()  # Initialiser result_image avec une copie de l'image actuelle\n",
    "            filtered_image = pre_processing(result_image)\n",
    "            \n",
    "            # Blob detector parameters for circle detection\n",
    "            params = cv2.SimpleBlobDetector_Params()\n",
    "            \n",
    "            # Create a blob detector with the configured parameters\n",
    "            detector = cv2.SimpleBlobDetector_create(params)\n",
    "            \n",
    "            # Detect blobs in the image\n",
    "            keypoints = detector.detect(filtered_image)\n",
    "            \n",
    "            # Update the info about the robot\n",
    "            robot_state, d = robot_info(keypoints, result_image,params)\n",
    "    \n",
    "            state_matrix, center_matrix, x_end, y_end =grid_setting(filtered_image, result_image)\n",
    "            \n",
    "            state_matrix = np.array(state_matrix)\n",
    "            data = symetrie_lignes(state_matrix)\n",
    "            for i in range(num_cases_y):\n",
    "                for j in range(num_cases_x):\n",
    "                    if (state_matrix[i][j] == None) or d>100: #and (x_end ==0 or y_end == 0)\n",
    "                        image_not_good = True\n",
    "        else:\n",
    "            print('no image found')\n",
    "            image_not_good = False\n",
    "            \n",
    "    return data, center_matrix, x_end, y_end, robot_state, result_image\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460b3d1-7b88-4677-bb7d-f7dba5580f8a",
   "metadata": {},
   "source": [
    "### 4.1. Pre-processing the raw image:\n",
    "\n",
    "   \n",
    "Our process commences with the acquisition of a live camera feed, upon which we conduct a series of filtration and computer vision operations, when calling '*pre_processing(image)*'. Initially, we convert the live feed into a grayscale image to standardize the input. Subsequently, we apply a bilateral filter to preserve edges while diminishing noise. Lastly, we enhance the contrast through Contrast Limited Adaptive Histogram Equalization (CLAHE), which equips us with a fully detailed grayscale image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d622a9-bf93-4997-8e88-e5f95f0f38cb",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap;\">\n",
    "    <div style=\"flex-basis: 50%; text-align: center;\">\n",
    "        <img src=\"image/raw_image.png\" alt=\"Step 1\" style=\"max-width: 90%;\"/>\n",
    "        <p><strong>Step 1: Raw Image</strong></p>\n",
    "    </div>\n",
    "    <div style=\"flex-basis: 50%; text-align: center;\">\n",
    "        <img src=\"image/image_grey.png\" alt=\"Step 2\" style=\"max-width: 90%;\"/>\n",
    "        <p><strong>Step 2: Grey Image</strong></p>\n",
    "    </div>\n",
    "    <div style=\"flex-basis: 50%; text-align: center;\">\n",
    "        <img src=\"image/bilateral_filtered.png\" alt=\"Step 3\" style=\"max-width: 90%;\"/>\n",
    "        <p><strong>Step 3: Bilateral Filtered</strong></p>\n",
    "    </div>\n",
    "    <div style=\"flex-basis: 50%; text-align: center;\">\n",
    "        <img src=\"image/clahe_applied.png\" alt=\"Step 4\" style=\"max-width: 90%;\"/>\n",
    "        <p><strong>Step 4: CLAHE Applie</strong></p>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0652b-3b11-4a89-b1c1-e7b09194e225",
   "metadata": {},
   "source": [
    "***Images at the different step of the pre-processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb7404-131e-426c-bb4c-c36697d01038",
   "metadata": {},
   "source": [
    "### 4.2. Identification of Robot State:\n",
    "\n",
    "Using the *robot_info(keypoints, result_image, params)* function, we analyze the image to identify key markers on the robot, specifically two distinct circles. The largest circle, positioned at the robot's center, determines its current coordinates. The relative positioning of the two circles enables us to infer the robot's orientation by comparing the vector formed by these circles against the horizontal axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf066a-2213-485b-b64e-518b85f75f7e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> \n",
    "    <img src=\"image/image_with_circles_drawn.png\" alt=\"environnement\" width=\"500\"/> \n",
    "    <p><strong>Figure 1:</strong> Robot with localisation circles</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcada1b-ef9c-42b1-82ec-65d2c7bbb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(image):\n",
    "    \"\"\"    \n",
    "    Preprocess an image for use in computer vision applications.\n",
    "    \n",
    "    Arguments:\n",
    "    image : The input image to be preprocessed.\n",
    "        \n",
    "    Returns:\n",
    "    filtered_image : The preprocessed image.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale for filtering.\n",
    "    image_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply bilateral filtering to reduce noise while preserving edges.\n",
    "    bilateral = cv2.bilateralFilter(image_grey, d=5, sigmaColor=25, sigmaSpace=25)\n",
    "    \n",
    "    # Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) for enhancing details.\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    filtered_image = clahe.apply(bilateral)\n",
    "    \n",
    "    return filtered_image \n",
    "\n",
    "def robot_info(keypoints, result_image,params):\n",
    "    global robot_state\n",
    "    \"\"\"\n",
    "    Update the essential information to describe the state of the robot.\n",
    "    \n",
    "    Arguments:\n",
    "        keypoints: Detected keypoints representing potential robot positions.\n",
    "        result_image: The image on which to draw the keypoints.\n",
    "        robot_state: An instance of RobotState containing the coordinates and angle of the robot.\n",
    "        \n",
    "    Returns:\n",
    "        robot_state: The updated RobotState instance.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Filter by Circularity\n",
    "    params.filterByCircularity = True\n",
    "    params.minCircularity = 0.8\n",
    "    params.maxCircularity = 1\n",
    "        \n",
    "    # Filter by color\n",
    "    params.filterByColor = True\n",
    "\n",
    "    x=y=r=x_big=y_big=r_big=x_small=y_small=r_small=0 \n",
    "    d = float('inf')\n",
    "    \n",
    "    # Minimum circle radius we want to detect\n",
    "    small_circle_radius_threshold_max = 14 \n",
    "    small_circle_radius_threshold_min = 5\n",
    "    \n",
    "    # Check if any blobs (potential circles) were detected\n",
    "    if keypoints:            \n",
    "        # Draw circles on the original image\n",
    "        for keypoint in keypoints:\n",
    "            x = int(keypoint.pt[0])\n",
    "            y = int(keypoint.pt[1])\n",
    "            r = int(keypoint.size / 2)\n",
    "\n",
    "            if small_circle_radius_threshold_min < r < small_circle_radius_threshold_max:\n",
    "                x_small = x\n",
    "                y_small = y\n",
    "                r_small = r\n",
    "            elif r > small_circle_radius_threshold_max:\n",
    "                x_big = x\n",
    "                y_big = y\n",
    "                r_big = r\n",
    "                \n",
    "        robot_state.x = x_big\n",
    "        robot_state.y = y_big\n",
    "        \n",
    "        robot_state.angle = - calculate_angle_degrees(x_big, y_big, x_small, y_small)\n",
    "\n",
    "        d= calculate_distance(x_small, y_small, x_big,y_big)\n",
    "\n",
    "    return robot_state, d \n",
    "\n",
    "def order_points(pts):\n",
    "    \"\"\"\n",
    "    Orders the corners of a rectangle or quadrilateral in a consistent way: \n",
    "    top-left, top-right, bottom-right, bottom-left. \n",
    "\n",
    "    Parameters:\n",
    "    - pts: A numpy array of four points (x, y)\n",
    "\n",
    "    Returns:\n",
    "    - A reordered numpy array of points in the order [top-left, top-right, bottom-right, bottom-left].\n",
    "    \"\"\"\n",
    "\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    s = pts.sum(axis=1)\n",
    "    diff = np.diff(pts, axis=1)\n",
    "\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "def grid_setting(filtered_image, result_image):\n",
    "    \"\"\"\n",
    "    Processes a filtered image to identify and analyze a grid, which is assumed to be the largest detected contour.\n",
    "\n",
    "    Parameters:\n",
    "    - filtered_image: A pre-processed image where contours can be easily detected, typically a grayscale or thresholded image.\n",
    "\n",
    "    Returns:\n",
    "    - The updated state matrix, center matrix, and the end coordinates of a specific goal within the grid.\n",
    "    \"\"\"\n",
    "    global center_matrix, state_matrix\n",
    "    #local variables used for the good detection of the different parts of the grid\n",
    "    BLACK = 1\n",
    "    WHITE = 0\n",
    "    black_threshold = 40\n",
    "    white_threshold = 101\n",
    "    red_threshold_inf = 41\n",
    "    red_threshold_sup = 90\n",
    "    robot_pos_delta_x = 55\n",
    "    robot_pos_delta_y = 55\n",
    "    min_area_threshold = 75000\n",
    "    \n",
    "    x_center = y_center = x_end = y_end = 0\n",
    "\n",
    "    contours_image = cv2.Canny(filtered_image, 50, 150, apertureSize=3)\n",
    "    contours, _ = cv2.findContours(contours_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    center_matrix_prev = copy.deepcopy(center_matrix)\n",
    "    state_matrix_prev = copy.deepcopy(state_matrix)\n",
    "\n",
    "    x_end_prev = x_end\n",
    "    y_end_prev = y_end\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        if cv2.contourArea(largest_contour) > min_area_threshold:\n",
    "            \n",
    "            # We first approximate the contour to a rectangle\n",
    "            peri = cv2.arcLength(largest_contour, True) #return the perimeter of our contour \n",
    "            approx = cv2.approxPolyDP(largest_contour, 0.02 * peri, True)  #return the number of the corners of the detected shape\n",
    "            #0.02* peri: This value represents the maximum distance between the original curve and its approximation\n",
    "            \n",
    "            if len(approx) == 4:\n",
    "                cv2.drawContours(result_image, [approx], -1, (0, 255, 0), 2)\n",
    "                \n",
    "                ordered_points = order_points(approx.reshape(4, 2))\n",
    "                destination_points = np.array([\n",
    "                    [0, 0],\n",
    "                    [filtered_image.shape[1] - 1, 0],\n",
    "                    [filtered_image.shape[1] - 1, filtered_image.shape[0] - 1],\n",
    "                    [0, filtered_image.shape[0] - 1]\n",
    "                ], dtype=\"float32\")\n",
    "                \n",
    "                matrix = cv2.getPerspectiveTransform(ordered_points, destination_points)\n",
    "                warped_image = cv2.warpPerspective(filtered_image, matrix, (filtered_image.shape[1], filtered_image.shape[0]))\n",
    "                inverse_matrix = cv2.invert(matrix)[1]\n",
    "                        \n",
    "                case_width = warped_image.shape[1] // num_cases_x\n",
    "                case_height = warped_image.shape[0] // num_cases_y\n",
    "\n",
    "                distortion_corr = np.array([0.9501, 0.9740, 0.9876, 1, 1.0064, 1.0092, 1.0152, 1.0164])\n",
    "                for i in range(num_cases_y):\n",
    "                    for j in range(num_cases_x):\n",
    "                        center_x = j * case_width * distortion_corr[j] + case_width // 2\n",
    "                        center_y = i * case_height + case_height // 2\n",
    "\n",
    "                        #conversion in a numpy array of type float32 as this format is required by OpenCV functions for transformations.\n",
    "                        float_center = np.array([[[center_x, center_y]]], dtype=np.float32)\n",
    "                        \n",
    "                        original_center = cv2.perspectiveTransform(float_center, inverse_matrix)\n",
    "                        original_center = tuple(original_center[0][0].astype(int))\n",
    "                        \n",
    "                        cv2.circle(result_image, original_center, 5, (0, 0, 255), -1)\n",
    "                        \n",
    "                        center_matrix[i][j] = original_center\n",
    "                        \n",
    "                        x_center, y_center = original_center  \n",
    "                        center_value = filtered_image[y_center, x_center]  \n",
    "                        debug_matrix[i][j] = center_value \n",
    "                        if center_value < black_threshold :\n",
    "                            if abs(x_center-robot_state.x) <= robot_pos_delta_x and abs(y_center-robot_state.y) <= robot_pos_delta_y:\n",
    "                                state_matrix[i][j] = WHITE\n",
    "                            else:\n",
    "                                state_matrix[i][j] = BLACK     \n",
    "                        if center_value > white_threshold:\n",
    "                            state_matrix[i][j] = WHITE\n",
    "                        if red_threshold_inf <= center_value <= red_threshold_sup:\n",
    "                            x_end, y_end = center_matrix[i][j]\n",
    "                            state_matrix[i][j] = WHITE\n",
    "    else:\n",
    "        center_matrix = copy.deepcopy(center_matrix_prev)\n",
    "        state_matrix = copy.deepcopy(state_matrix_prev) \n",
    "        x_end = x_end_prev\n",
    "        y_end = y_end_prev\n",
    "        \n",
    "    return state_matrix, center_matrix, x_end, y_end\n",
    "\n",
    "def info_robot_cam():\n",
    "    \"\"\"\n",
    "    Update the essential informations to describe the state of the robot and of the map.\n",
    "    \n",
    "    Arguments:\n",
    "       \n",
    "    Returns:\n",
    "        robot_state: The updated RobotState instance.\n",
    "        center_matrix:\n",
    "        x_end, y_end:\n",
    "        result_image: Image with all the analysis \n",
    "    \"\"\"\n",
    "    #global robot_state, result_image, center_matrix, state_matrix,x_end,y_end, data\n",
    "    global center_matrix, state_matrix\n",
    "    image_not_good = True\n",
    "    while image_not_good: \n",
    "        image_not_good = False\n",
    "        ret, frame = cam.read()\n",
    "        if ret:\n",
    "            result_image = frame.copy()  # Initialiser result_image avec une copie de l'image actuelle\n",
    "            filtered_image = pre_processing(result_image)\n",
    "             # Blob detector parameters for circle detection\n",
    "            params = cv2.SimpleBlobDetector_Params()\n",
    "            \n",
    "            # Create a blob detector with the configured parameters\n",
    "            detector = cv2.SimpleBlobDetector_create(params)\n",
    "            \n",
    "            # Detect blobs in the image\n",
    "            keypoints = detector.detect(filtered_image)\n",
    "            \n",
    "            # Update the info about the robot\n",
    "            robot_state, d = robot_info(keypoints, result_image,params)\n",
    "    \n",
    "            state_matrix, center_matrix, x_end, y_end =grid_setting(filtered_image, result_image)\n",
    "            \n",
    "            state_matrix = np.array(state_matrix)\n",
    "            data = symetrie_lignes(state_matrix)\n",
    "            for i in range(num_cases_y):\n",
    "                for j in range(num_cases_x):\n",
    "                    if (state_matrix[i][j] == None) or d>100: #and (x_end ==0 or y_end == 0)\n",
    "                        image_not_good = True\n",
    "        else:\n",
    "            print('no image found')\n",
    "            image_not_good = False\n",
    "            \n",
    "\n",
    "    return data, center_matrix, x_end, y_end, robot_state, result_image\n",
    "\n",
    "\n",
    "def cam_get_pos():\n",
    "    global robot_state\n",
    "    \"\"\"\n",
    "    Same principle as robot_info() but here to prevent the case where the camera is blocked\n",
    "    \n",
    "    Arguments:\n",
    "        keypoints: Detected keypoints representing potential robot positions.\n",
    "        result_image: The image on which to draw the keypoints.\n",
    "        robot_state: An instance of RobotState containing the coordinates and angle of the robot.\n",
    "        \n",
    "    Returns:\n",
    "        robot_state: The updated RobotState instance.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Filter by Circularity\n",
    "    params.filterByCircularity = True\n",
    "    params.minCircularity = 0.8\n",
    "    params.maxCircularity = 1\n",
    "        \n",
    "    # Filter by color\n",
    "    params.filterByColor = True\n",
    "\n",
    "    x=y=r=x_big=y_big=r_big=x_small=y_small=r_small=0 \n",
    "    d = float('inf')\n",
    "    \n",
    "    # Minimum circle radius we want to detect\n",
    "    small_circle_radius_threshold_max = 14 \n",
    "    small_circle_radius_threshold_min = 5\n",
    "    \n",
    "    # Check if any blobs (potential circles) were detected\n",
    "    if keypoints:            \n",
    "        # Draw circles on the original image\n",
    "        for keypoint in keypoints:\n",
    "            x = int(keypoint.pt[0])\n",
    "            y = int(keypoint.pt[1])\n",
    "            r = int(keypoint.size / 2)\n",
    "\n",
    "            if small_circle_radius_threshold_min < r < small_circle_radius_threshold_max:\n",
    "                x_small = x\n",
    "                y_small = y\n",
    "                r_small = r\n",
    "            elif r > small_circle_radius_threshold_max:\n",
    "                x_big = x\n",
    "                y_big = y\n",
    "                r_big = r\n",
    "                \n",
    "        robot_state.x = x_big\n",
    "        robot_state.y = y_big\n",
    "        \n",
    "        robot_state.angle = - calculate_angle_degrees(x_big, y_big, x_small, y_small)\n",
    "\n",
    "        d= calculate_distance(x_small, y_small, x_big,y_big)\n",
    "    else:\n",
    "        robot_state.x = 0\n",
    "        robot_state.y = 0\n",
    "        robot_state.angle =0\n",
    "    return robot_state, d "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920bbd7-48a8-4fad-a251-c311d9ee64c0",
   "metadata": {},
   "source": [
    "### 4.3. Grid Analysis:\n",
    " \n",
    "We employ the grid_setting(filtered_image) function to define or update the state and center matrices. By recognizing the largest contour, which outlines our operational grid, and by locating its four corners, we ascertain the pixel positions of each cell's center, thus updating our center_matrix. Subsequently, by examining the intensity of the centers, we differentiate between occupied and vacant cells, updating our state_matrix accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ef92-5fea-43f2-b8ce-f96bb570baab",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> \n",
    "    <img src=\"image/image_with_red_points.png\" alt=\"environnement\" width=\"500\"/> \n",
    "    <p><strong>Figure 2:</strong> Map with the red points in the center of the cells</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66498e58-9253-4d4e-aae3-d90d5d18d01e",
   "metadata": {},
   "source": [
    "As our map is in a large format: A0, since we are taking the photo from above the map, there is a slight distortion on the sides which means that when the robot is on a square, its center does not match the center below it. See '*Figure 4*'. That's why we have calibrated a matrix that puts all the points in the right places. We placed the robot in the center of a square for each column and compared the value of the robot's center in x with the theoretical center: '*Figure 3*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d0d6a-ae21-4204-84c1-44937aecaafc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> \n",
    "    <img src=\"image/image_with_corrected_red_points.png\" alt=\"environnement\" width=\"500\"/> \n",
    "    <p><strong>Figure 3:</strong> image_with_red_points</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd656757-3390-45b1-a253-fd07f2ec744e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> \n",
    "    <img src=\"image/schema_vision.jpg\" alt=\"environnement\" width=\"300\"/> \n",
    "    <p><strong>Figure 4:</strong> Diagram of Image Correction</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00619d-92e7-4de2-806e-48b9ae84a15c",
   "metadata": {},
   "source": [
    "## 5. Motor Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c55b5-d3cf-40ca-b39c-b180be126241",
   "metadata": {},
   "source": [
    "As our robot advance and follow the path calculated by the global navigation, our robot sometimes deviate. We decided that a correction fonction is needed to assure the global trajectory. The first idea was to create a PID to follow the measured angle. But we couldn't apply the PID to our algoritm. That's why we create a second version to control specificly the direction correction.\n",
    "\n",
    "After every cell the robot moved to, we check if its in the expected orientation. If the difference $d\\theta$ between its orientation and the expected one is larger than 5 degrees, we correct it:\n",
    "\n",
    "- if $d\\theta < 5$: we do not modify it\n",
    "- if $5<d\\theta < 15$: we orientate it to have the expected orientation \n",
    "- if $d\\theta > 15$: we orientate it to aim on the next cell's center\n",
    "\n",
    "Moreover, if the robot just had to avoid an obstacle using local navigation, we orientate it in the direction of the second next cell; like this, it wont try to go in the celle the obstacle is still in. \n",
    "\n",
    "We call the controller everytime the robot made it safely to a cell, everytime we (re)compute A* for it to start with orientation 0, and just after we avoided an obstacle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5536438-4098-49b7-9ab5-58478b2c5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_robot(client):\n",
    "    '''\n",
    "    Rotates the robot by calculating the required rotation angle and executing the rotation.\n",
    "    '''\n",
    "    global x1 ,y1 ,x2 ,y2, motor_left_target, motor_right_target,angle,move_list,count\n",
    "    angle = calculate_angle_degrees(x1, y1, x2, y2)\n",
    "    normalize_angle(angle)\n",
    "    if abs(move_list[count] - angle) < 5:\n",
    "        angle = 0\n",
    "    else :\n",
    "        if count+1 < len(move_list):\n",
    "            move_list[count+1] = angle\n",
    "        angle = angle - move_list[count]\n",
    "        count = count+1\n",
    "    \n",
    "    if angle < 0 :\n",
    "        angle = -angle\n",
    "        time_rotation = (4.95 * angle) / 180\n",
    "        v = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(-100)],\n",
    "        }\n",
    "        aw(node.set_variables(v))\n",
    "        time.sleep(time_rotation)\n",
    "        filter_call(100, -100, time_rotation)\n",
    "\n",
    "    else:\n",
    "        time_rotation = (4.95 * angle) / 180\n",
    "        if time_rotation == 0 :\n",
    "            v_reverse = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(100)],\n",
    "            }\n",
    "            filter_call(100, 100, time_rotation)\n",
    "\n",
    "        else :\n",
    "            v_reverse = {\n",
    "                \"motor.left.target\": [int(-100)],\n",
    "                \"motor.right.target\": [int(100)],\n",
    "            }\n",
    "            filter_call(-100, 100, time_rotation)\n",
    "\n",
    "        aw(node.set_variables(v_reverse))\n",
    "        time.sleep(time_rotation)\n",
    "    v_stop = {\n",
    "            \"motor.left.target\": [int(0)],\n",
    "            \"motor.right.target\": [int(0)],\n",
    "        }\n",
    "    aw(node.set_variables(v_stop))\n",
    "     \n",
    "def move_robot(client):\n",
    "    global x1 ,y1 ,x2 ,y2, motor_left_target, motor_right_target,distance\n",
    "    distance = calculate_distance(x1, y1, x2, y2)\n",
    "    time_distance = distance*1.6\n",
    "    v = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(100)],\n",
    "    }\n",
    "    aw(node.set_variables(v))\n",
    "    time.sleep(time_distance)\n",
    "    filter_call(100, 100, time_distance)\n",
    "    v_stop = {\n",
    "            \"motor.left.target\": [int(0)],\n",
    "            \"motor.right.target\": [int(0)],\n",
    "        }\n",
    "    aw(node.set_variables(v_stop))\n",
    "\n",
    "def motor_set_speed(left_speed, right_speed,node):\n",
    "    aw(node.set_variables(motors(left_speed,right_speed)))\n",
    "    \n",
    "def motors(left_speed, right_speed):\n",
    "    '''\n",
    "    Creates a dictionary of motor speed targets for left and right motors.\n",
    "    '''\n",
    "    return {\n",
    "        \"motor.left.target\": [left_speed],\n",
    "        \"motor.right.target\": [right_speed],\n",
    "    }\n",
    "    \n",
    "def check_obstacle_state(obst):\n",
    "    \"\"\"Checks which next state the robot should go in\n",
    "        0: saw an obstacle\n",
    "        1: normal state\n",
    "        2: corrects trajectory after avoiding obstacle\n",
    "    \"\"\"\n",
    "\n",
    "    global saw_obstacle\n",
    "\n",
    "    obstThr = 500\n",
    "    if all(elements < obstThr for elements in obst):\n",
    "        return 1  # initial state, no obstacle\n",
    "    else:\n",
    "        return 0  # Obstacle detected\n",
    "\n",
    "def move_robot_rect(client,x_a,y_a,x_t,y_t):\n",
    "    global motor_left_target, motor_right_target,distance\n",
    "    x_a = x_a / 100\n",
    "    y_a = y_a / 100\n",
    "    x_t = x_t / 100\n",
    "    y_t = y_t / 100\n",
    "    distance = calculate_distance(x_a, y_a, x_t, y_t)\n",
    "    time_distance = distance*4.5\n",
    "    v = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(100)],\n",
    "    }\n",
    "    aw(node.set_variables(v))\n",
    "    time.sleep(time_distance)\n",
    "    filter_call(100, 100, time_distance)\n",
    "    v_stop = {\n",
    "            \"motor.left.target\": [int(0)],\n",
    "            \"motor.right.target\": [int(0)],\n",
    "        }\n",
    "    aw(node.set_variables(v_stop))\n",
    "\n",
    "def rotate_robot_angle(client, angle):\n",
    "    '''\n",
    "    Rotates the robot to a specific angle.\n",
    "    '''\n",
    "    global motor_left_target, motor_right_target,move_list,count, ekf\n",
    "    \n",
    "    if angle < 0 :\n",
    "        angle = - angle\n",
    "        time_rotation = (4.95 * angle) / 180\n",
    "        v = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(-100)],\n",
    "    }\n",
    "        aw(node.set_variables(v))\n",
    "        time.sleep(time_rotation)\n",
    "        filter_call(100, -100, time_rotation)\n",
    "        \n",
    "    else:\n",
    "        time_rotation = (4.95 * angle) / 180\n",
    "        if time_rotation == 0 :\n",
    "            v_reverse = {\n",
    "            \"motor.left.target\": [int(100)],\n",
    "            \"motor.right.target\": [int(100)],\n",
    "            }\n",
    "            aw(node.set_variables(v_reverse))\n",
    "            time.sleep(time_rotation)\n",
    "            filter_call(100, 100, time_rotation)\n",
    "        else :\n",
    "            v_reverse = {\n",
    "                \"motor.left.target\": [int(-100)],\n",
    "                \"motor.right.target\": [int(100)],\n",
    "            }\n",
    "            aw(node.set_variables(v_reverse))\n",
    "            time.sleep(time_rotation)\n",
    "            filter_call(-100, 100, time_rotation)\n",
    "        \n",
    "    v_stop = {\n",
    "            \"motor.left.target\": [int(0)],\n",
    "            \"motor.right.target\": [int(0)],\n",
    "        }\n",
    "    aw(node.set_variables(v_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc7557-3a83-4933-a666-b38c603a5044",
   "metadata": {},
   "source": [
    "## 6. Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de855e-4d85-4ac7-aac8-7c16f2a2e94a",
   "metadata": {},
   "source": [
    "To be sure that we have the correct position of the robot even when the camera fails, we implemented a filtering feature. As our model was not linear, we chose the Extended Kalman Filter. This filter takes as input the position measured by the camera, and the instaneous speed of the robot. Every time it is called, it updates the position based on the previous position, the camera input, and the model of the robot (where it is supposed to be given the speed, the time elapsed, and the previous position).\n",
    "\n",
    "We call the filter after every movement, giving the time period and the speed, thanks to the method `filter_call()` below.\n",
    "\n",
    "The rest of the code is all in ``ExtendedKalmanFilter.py``\n",
    "\n",
    "The key elements of the EKF are:\n",
    "\n",
    "#### State Space Model\n",
    "\n",
    "The system is modeled using a state-space representation. The state at time step $k$ is denoted by $x_k$, and the system dynamics are represented by two matrices: $A$ (representing the system if the robot is not moving) and $B$ representing the system if the robot is moving with speed $u_k$. B is defined by a method B(orientation, dt).\n",
    "The model in movement is then computed by:\n",
    "\n",
    "$x_k = A \\cdot (x_{k-1}) + (B(orientation,dt)) \\cdot (u_{k-1}) + (v_{k-1})$\n",
    "\n",
    "Here, $(u_{k-1})$ is the control input, (in our case, the linear speed and the angular speed) and $v_{k-1}$ is the process noise. We chose $(v_{k-1} = [TODO])$ for our model not to have too much weight.\n",
    "\n",
    "#### Measurement Model\n",
    "\n",
    "Measurements $z_k$ are related to the state $x_k$ through a measurement function $h$, with added measurement noise $v_k$:\n",
    "\n",
    "$ z_k = h(x_k) + v_k $. Here, our measurements are the camera inputs for the position in x,y of the robot (in pixels on the map) and its orientation (in degrees, with 0 being the right side of the map, and positive in the trigonometric direction). Our measurement noise matrix is $v_k = [TODO]$. \n",
    "\n",
    "We also have a matrix $R$, the measurement noise covariance matrix, and the matrix $R_{no cam}$, which is used instead of R when the camera gives no input.\n",
    "We chose this way in order to have the measurements weight a lot in the computations, but then way less when the camera is blocked, and being able to estimated correctly the position.\n",
    "#### Prediction Step\n",
    "\n",
    "In the prediction step, the EKF estimates the new state based on the previous state estimate and the system dynamics. Here is the predict step:\n",
    "\n",
    "$x_k$ computed as explained above\n",
    "$P_k = A_{k-1} \\cdot P_{k-1} \\cdot A_{k-1}^T + (Q_k)$\n",
    "\n",
    "Where:\n",
    "- $P_k^-$ is the predicted state covariance matrix.\n",
    "- $Q_{k-1}$ is the process noise covariance matrix.\n",
    "\n",
    "#### Update Step\n",
    "\n",
    "In the update step, the predicted state is corrected based on the measurements:\n",
    "\n",
    "$y_k = z_k - ((H_k \\cdot x_k) + (w_k))$\n",
    "\n",
    "$S_k = H_k \\cdot P_k \\cdot H_k^T + R_k$ (choosing the correct R matrix according to the camera input)\n",
    "\n",
    "$K_k = P_k \\cdot H_k^T \\cdot pinv(S_k)$\n",
    "\n",
    "$x_k = (x_k + (K_k \\cdot y_k))$\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\(K_k\\) is the Kalman Gain.\n",
    "- np.linalg.pinv is the method to compute the Moore-Penrose pseudo inverse of a matrix\n",
    "\n",
    "We then update the new $P_k$ and $x_k$ to be ready for the next step.\n",
    "Our estimated position is the last x_k computed!  \n",
    "\n",
    "We use this position in the corrector, for our robot to have good control even without the measurements from the camera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267baab-3e1b-44e3-9c8b-94c7715447dc",
   "metadata": {},
   "source": [
    "```python \n",
    "def filter_call(left, right, dt):\n",
    "    \"\"\"Makes the call to the filter using the initialised ekf\n",
    "        Args:\n",
    "        right: right speed of the motor in thymio unit (same unit as we give the motor)\n",
    "        left: left speed\n",
    "        Returns: estimated positions x, y, angle in pixels, degrees\n",
    "        \"\"\"\n",
    "    global ekf\n",
    "    #dt = update_time()\n",
    "    speed = get_speed(right, left)\n",
    "    _, _, _, _, position, _ = info_robot_cam()\n",
    "    cam_pos = np.array([position.x, position.y, position.angle])\n",
    "    return ekf.filter(cam_pos, speed, dt)\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2d203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_time():\n",
    "    \"\"\"Updates the last time checkpoint and returns the time spent since\"\"\"\n",
    "    global last_time\n",
    "    time_now = time.monotonic()\n",
    "    dt = time_now-last_time\n",
    "    last_time = time_now\n",
    "    return dt\n",
    "\n",
    "def filter_call(left, right, dt):\n",
    "    \"\"\"Makes the call to the filter using the initialised ekf\n",
    "        Args:\n",
    "        right: right speed of the motor in thymio unit (same unit as we give the motor)\n",
    "        left: left speed\n",
    "        Returns: estimated positions x, y, angle in pixels, degrees\n",
    "        \"\"\"\n",
    "    global ekf\n",
    "    #dt = update_time()\n",
    "    speed = get_speed(right, left)\n",
    "    _, _, _, _, position, _ = info_robot_cam()\n",
    "    cam_pos = np.array([position.x, position.y, position.angle])\n",
    "    return ekf.filter(cam_pos, speed, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fd0f2-4a19-4ce4-9843-3d947a6b701d",
   "metadata": {},
   "source": [
    "## 7. Global Localisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57ffe8-ab1d-483e-8891-f5fa3af40b09",
   "metadata": {},
   "source": [
    "This module is very dependant on the vision module, since the vision module maps the area. The vision module returns a state matrix, giving us the information on which square is occupied or not. After manipulating this matrix so it can work conviniently with the function that runs with the A* function, we can run the algorithm. The shortest path is then given to us in the form of a double array, called in the code `path`.\n",
    "\n",
    "We have chosen the A* algorithm because our sceario, which is a robot trying to find the goal in a 'maze', corresponds exactly on how we want the shortest path be computed from. Using the heuristic function h wasn't really necessary since all the distances the robot runs have an equal distance from each other. But in the case where our robot can use 8 directions paths, it would've helped in fiding the shortest path.\n",
    "\n",
    "Once `path` is filled, we run a loop that will make the robot move to each location one by one, starting by loading the values of its current position and the next one : \n",
    "\n",
    "            for i in range(len(path)-1):\n",
    "                x1, y1 = path[i]\n",
    "                x2, y2 = path[i + 1]\n",
    "\n",
    "\n",
    "We make the robot move three times to move from a location to another, so it has the time between each movement to check the obstacles, and update the `obstacle_state` variable. If the robot encounters an obstacle, it will run another loop that makes the robot avoid the obstacle (see Local Navigation section).\n",
    "\n",
    "<div style=\"text-align: center;\"> \n",
    "    <img src=\"image/screen_path_1.png\" alt=\"global_nav\" width=\"500\"/> \n",
    "    <p><strong>Figure 5:</strong> Path planning execution, starts from green dot and plans execution to the purple dot </p> \n",
    "</div>\n",
    "\n",
    "All the movements that the robot makes are computed via mathematical functions, (namely, `calculate_angle_degrees` and `calculate_distance`), and given to `move_robot` and `rotate_robot`, who will give a command to a motor and put to sleep using `time.sleep()` during the time the command need to be applied.\n",
    "\n",
    "Then, we check if the robot has reached its goal by looking at the robot position and the goal position via the camera. If the two positions matches (with a margin error around 50 pixels), then we stop the program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7280099a-cd18-464a-beab-541e0954a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_plot(num_cases_x,num_cases_y):\n",
    "    \"\"\"\n",
    "    Helper function to create a figure of the desired dimensions & grid\n",
    "    \n",
    "    :param max_val: dimension of the map along the x and y dimensions\n",
    "    :return: the fig and ax objects.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    \n",
    "    major_ticks = np.arange(0, num_cases_x+1, 1)\n",
    "    minor_ticks = np.arange(0, num_cases_y+1, 1)\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.set_yticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', alpha=0.2)\n",
    "    ax.grid(which='major', alpha=0.5)\n",
    "    ax.set_ylim([-1,num_cases_y])\n",
    "    ax.set_xlim([-1,num_cases_x])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def _get_movements_4n():\n",
    "    \"\"\"\n",
    "    Get all possible 4-connectivity movements (up, down, left right).\n",
    "    :return: list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0)]\n",
    "\n",
    "def _get_movements_8n():\n",
    "    \"\"\"\n",
    "    Get all possible 8-connectivity movements. Equivalent to get_movements_in_radius(1)\n",
    "    (up, down, left, right and the 4 diagonals).\n",
    "    :return: list of movements with cost [(dx, dy, movement_cost)]\n",
    "    \"\"\"\n",
    "    s2 = math.sqrt(2)\n",
    "    return [(1, 0, 1.0),\n",
    "            (0, 1, 1.0),\n",
    "            (-1, 0, 1.0),\n",
    "            (0, -1, 1.0),\n",
    "            (1, 1, s2),\n",
    "            (-1, 1, s2),\n",
    "            (-1, -1, s2),\n",
    "            (1, -1, s2)]\n",
    "\n",
    "def reconstruct_path(cameFrom, current):\n",
    "    \"\"\"\n",
    "    Recurrently reconstructs the path from start node to the current node\n",
    "    :param cameFrom: map (dictionary) containing for each node n the node immediately \n",
    "                     preceding it on the cheapest path from start to n \n",
    "                     currently known.\n",
    "    :param current: current node (x, y)\n",
    "    :return: list of nodes from start to current node\n",
    "    \"\"\"\n",
    "    total_path = [current]\n",
    "    while current in cameFrom.keys():\n",
    "        # Add where the current node came from to the start of the list\n",
    "        total_path.insert(0, cameFrom[current]) \n",
    "        current=cameFrom[current]\n",
    "        \n",
    "    return total_path\n",
    "\n",
    "def A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"4N\", num_cases_x=num_cases_x, num_cases_y=num_cases_y):\n",
    "    \"\"\"\n",
    "    A* for 2D occupancy grid. Finds a path from start to goal.\n",
    "    h is the heuristic function. h(n) estimates the cost to reach goal from node n.\n",
    "    :param start: start node (x, y)\n",
    "    :param goal_m: goal node (x, y)\n",
    "    :param occupancy_grid: the grid map\n",
    "    :param movement: select between 4-connectivity ('4N') and 8-connectivity ('8N', default)\n",
    "    :return: a tuple that contains: (the resulting path in meters, the resulting path in data array indices)\n",
    "    \"\"\"\n",
    "    # Check if the start and goal are within the boundaries of the map\n",
    "    for point in [start, goal]:\n",
    "        x_coord, y_coord = point \n",
    "\n",
    "        assert 0 <= x_coord < num_cases_x, \"X-coordinate of start or end goal not contained in the map\"\n",
    "        assert 0 <= y_coord < num_cases_y, \"Y-coordinate of start or end goal not contained in the map\"\n",
    "    \n",
    "    # check if start and goal nodes correspond to free spaces\n",
    "    if occupancy_grid[start[0], start[1]]:\n",
    "        raise Exception('Start node is not traversable')\n",
    "\n",
    "    if occupancy_grid[goal[0], goal[1]]:\n",
    "        raise Exception('Goal node is not traversable')\n",
    "    \n",
    "    # get the possible movements corresponding to the selected connectivity\n",
    "    if movement_type == '4N':\n",
    "        movements = _get_movements_4n()\n",
    "    elif movement_type == '8N':\n",
    "        movements = _get_movements_8n()\n",
    "    else:\n",
    "        raise ValueError('Unknown movement')\n",
    " \n",
    "    # The set of visited nodes that need to be (re-)expanded, i.e. for which the neighbors need to be explored\n",
    "    # Initially, only the start node is known.\n",
    "    openSet = [start]\n",
    "    \n",
    "    # The set of visited nodes that no longer need to be expanded.\n",
    "    closedSet = []\n",
    "\n",
    "    # For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start to n currently known.\n",
    "    cameFrom = dict()\n",
    "\n",
    "    # For node n, gScore[n] is the cost of the cheapest path from start to n currently known.\n",
    "    gScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    gScore[start] = 0\n",
    "\n",
    "    # For node n, fScore[n] := gScore[n] + h(n). map with default value of Infinity\n",
    "    fScore = dict(zip(coords, [np.inf for x in range(len(coords))]))\n",
    "    fScore[start] = h[start]\n",
    "\n",
    "    # while there are still elements to investigate\n",
    "    while openSet != []:\n",
    "        \n",
    "        #the node in openSet having the lowest fScore[] value\n",
    "        fScore_openSet = {key:val for (key,val) in fScore.items() if key in openSet}\n",
    "        current = min(fScore_openSet, key=fScore_openSet.get)\n",
    "        del fScore_openSet\n",
    "        \n",
    "        #If the goal is reached, reconstruct and return the obtained path\n",
    "        if current == goal:\n",
    "#            print(closedSet)\n",
    "            return reconstruct_path(cameFrom, current), closedSet\n",
    "\n",
    "        openSet.remove(current)\n",
    "        closedSet.append(current)\n",
    "        \n",
    "        #for each neighbor of current:\n",
    "        for dx, dy, deltacost in movements:\n",
    "            \n",
    "            neighbor = (current[0]+dx, current[1]+dy)\n",
    "            \n",
    "            # if the node is not in the map, skip\n",
    "            if (neighbor[0] >= occupancy_grid.shape[0]) or (neighbor[1] >= occupancy_grid.shape[1]) or (neighbor[0] < 0) or (neighbor[1] < 0):\n",
    "                continue\n",
    "            \n",
    "            # if the node is occupied or has already been visited, skip\n",
    "            if (occupancy_grid[neighbor[0], neighbor[1]]) or (neighbor in closedSet): \n",
    "                continue\n",
    "                \n",
    "            # d(current,neighbor) is the weight of the edge from current to neighbor\n",
    "            # tentative_gScore is the distance from start to the neighbor through current\n",
    "            tentative_gScore = gScore[current] + deltacost\n",
    "            \n",
    "            if neighbor not in openSet:\n",
    "                openSet.append(neighbor)\n",
    "                \n",
    "            if tentative_gScore < gScore[neighbor]:\n",
    "                # This path to neighbor is better than any previous one. Record it!\n",
    "                cameFrom[neighbor] = current\n",
    "                gScore[neighbor] = tentative_gScore\n",
    "                fScore[neighbor] = gScore[neighbor] + h[neighbor]\n",
    "\n",
    "    # Open set is empty but goal was never reached\n",
    "    print(\"No path found to goal\")\n",
    "    return [], closedSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ac5d6-fa0b-48ce-b04e-b8902fdd667b",
   "metadata": {},
   "source": [
    "## 8. Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927692e-4b72-4a7f-8449-0d4aebb2d47b",
   "metadata": {},
   "source": [
    "### Theory : \n",
    "\n",
    "To ensure obstacle avoidance, we have opted for a convinient way to do so by using the Artificial Neural Network method. By using the five sensors in front of the Thymio, the robot can modify the speed of its wheel depending on the value returned by the thymio. We can use the following equation to compute the speed sent to the motors :\n",
    "\n",
    "\\begin{array}{rcl}\n",
    "v_{Left} = w_{1l}x_1 + w_{2l}x_2 + w_{3l}x_3 + w_{4l}x_4 + w_{5l}x_5 \\\\\n",
    "v_{Right} = w_{1r}x_1 + w_{2r}x_2 + w_{3r}x_3 + w_{4r}x_4 + w_{5r}x_5\n",
    "\\end{array}\n",
    "\n",
    "Thanks to the ANN method, ach proximity sensor has one weight coefficient. The coefficients $x_1, \\cdots, x_5$ are used as the measurements of the horizontal distances that the robot can sense.\n",
    "\n",
    "#### Obstacle detection using ANN : \n",
    "\n",
    "    speed0 = 50      \n",
    "    SpeedGainObst = [6, 4, -2, -6, -8]\n",
    "    SpeedLeftWheel = speed0\n",
    "    SpeedRightWheel = speed0\n",
    "\n",
    "    for i in range(5):\n",
    "        speedLeftWheel += obst[i] * SpeedGainObst[i] // 100\n",
    "        speedRightWheel += obst[i] * SpeedGainObst[4 - i] // 100\n",
    "\n",
    "    return speedLeftWheel, speedRightWheel\n",
    "\n",
    "### How to find the weights? \n",
    "\n",
    "In our code, we used the following weights : $[w_{1l}, w_{2l}, w_{3l}, w_{4l}, w_{5l}]=[0.06,0.04,-0.02,-0.06,-0.08]$ and $[w_{1r}, w_{2r}, w_{3r}, w_{4r}, w_{5r}]=[-0.08,-0.06,-0.02,0.04,0.06]$, by using a nominal speed of 100, and all of this by testing and correcting our values.  \n",
    "\n",
    "These coefficients are used so that the obstacle is dodged in a reasonnable manner (we want to stay in the velocity bound, and not give an angle which is too tight or too wide).\n",
    "\n",
    "### Switching from local navigation to global navigation : \n",
    "\n",
    "In order to be able to return in the main state after making the global navigation, we use a function that checks the robot state (state_robot). When all the sensor do not detect something in front of them, and has been detecting obstacles before, the state becomes 2. When the case is the same but the sensor has never been detecting something before, the state remains 1. And finally, when the sensor detects and obstacle, we enter in the \"obstacle avoidance\" mode, which is described by the state 0.\n",
    "\n",
    "### Kidnapping \n",
    "\n",
    "The objective of the kidnapping prevention mechanism for the Thymio robot is to detect and respond to instances where the robot is lifted off the ground, signaling a potential kidnapping scenario. This is achieved through continuous monitoring of the ground sensor, which detects changes in the robot's contact with the ground surface.\n",
    "\n",
    "The Thymio robot is equipped with a ground sensor that provides real-time information about the contact between the robot and the ground. The detection mechanism relies on regular checks of the ground sensor readings during the robot's movement. The code continuously monitors the sensor output to ensure that the robot is in contact with the ground. If a deviation from the expected sensor values occurs, indicating that the robot is no longer on the ground, the system interprets this as a potential kidnapping event.\n",
    "\n",
    "Upon detecting that the robot is not on the ground, the code initiates a predefined response strategy. The chosen approach involves resetting the robot's state and restarting the code from the beginning. We use the variable `was_kidnapped` to ensure that we can distinguish the case where the robot needs to restart or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe9abe0-7939-44d6-b6c3-6996945398bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kidnapped_state(sensor, state):\n",
    "    global was_kidnapped\n",
    "    tresh = 50\n",
    "    if all(elements < tresh for elements in sensor):\n",
    "        was_kidnapped = True\n",
    "        return 3\n",
    "    if was_kidnapped: # set to reinitialise\n",
    "        was_kidnapped = False\n",
    "        return 2\n",
    "    return state\n",
    "\n",
    "def local_navi(obst):\n",
    "\n",
    "    speed0 = 50      \n",
    "    obstSpeedGain = [6, 4, -2, -6, -8]\n",
    "    spLeft = speed0\n",
    "    spRight = speed0\n",
    "\n",
    "    for i in range(5):\n",
    "        spLeft += obst[i] * obstSpeedGain[i] // 100\n",
    "        spRight += obst[i] * obstSpeedGain[4 - i] // 100\n",
    "\n",
    "    return spLeft, spRight\n",
    "\n",
    "\n",
    "def correction_1(teta_a,x_a,y_a,x_t,y_t, teta_t):\n",
    "    teta = ( normalize_angle(teta_t - teta_a))\n",
    "    print(\"correction teta\", teta)\n",
    "    rotate_robot_angle(client, teta)\n",
    "\n",
    "def correction(x_a, y_a, teta_a, x_t,y_t,teta_t, x_t1, y_t1):\n",
    "    \"\"\"Controller that decides what correction to make depending on the distance and angle of estimated\"\"\"\n",
    "    dist = calculate_distance(x_a, y_a, x_t, y_t)\n",
    "    # if teta_a < -165: #-165\n",
    "    #    teta_a = -teta_a\n",
    "    if ((abs(teta_a-teta_t)%360 >5) and (abs(teta_a-teta_t)%360 <350)):\n",
    "        if obstacle_state == 2:\n",
    "            print(\"correction 1.3\")\n",
    "            correction_1(teta_a, x_a,y_a,x_t,y_t,teta_t)\n",
    "            move_robot_rect(client,x_a,y_a,x_t,y_t)\n",
    "        elif abs(x_a-x_t)>15 or abs(y_a-y_t)>15:\n",
    "            print(\"correction 1.1\")\n",
    "            correction_1(teta_a, x_a,y_a,x_t1,y_t1,teta_t)\n",
    "        else:\n",
    "            print(\"correction 1.2\")\n",
    "            correction_1(teta_a, x_a,y_a,x_t,y_t,teta_t)\n",
    "        return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1570848a-4770-47d3-bdb0-c6b6912af1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_star_move(client):\n",
    "    '''\n",
    "    Executes the A* pathfinding algorithm and moves the robot along the calculated path.\n",
    "    This function continuously checks for obstacles, and if detected, performs local navigation\n",
    "    to avoid them. It also checks if the robot is kidnapped (displaced unexpectedly) and responds accordingly.\n",
    "    '''\n",
    "    global x1, y1, x2, y2, motor_left_target, motor_right_target, move_list,obstacle_state,prox_ground_delta, prox_horizontal,obst, obstThrH, obstThrL, obstSpeedGain, speed0, speedGain, robot_state, saw_obstacle, x_end, was_kidnapped, ekf\n",
    "    goal = None\n",
    "    data, center_matrix, x_end, y_end, robot_state, result_image = info_robot_cam()\n",
    "\n",
    "\n",
    "    while True :\n",
    "        if obstacle_state == 1 :\n",
    "            _, _, _, _, robot_state, _ = info_robot_cam()\n",
    "\n",
    "            if ekf == None:\n",
    "                position = np.array([robot_state.x, robot_state.y, -robot_state.angle])\n",
    "                ekf = EKF(0.1, position, np.array([0,0]))\n",
    "            fig, ax = create_empty_plot(num_cases_x,num_cases_y)\n",
    "             # Creating the occupancy grid\n",
    "            center_matrix_array = np.array(center_matrix)\n",
    "            center_matrix_array = symetrie_lignes(center_matrix_array)\n",
    "                \n",
    "            cmap = colors.ListedColormap(['white', 'red']) # Select the colors with which to display obstacles and free cells    \n",
    "                \n",
    "            # Converting the random values into occupied and free cells\n",
    "            limit = 0\n",
    "            occupancy_grid = data.copy()\n",
    "    \n",
    "            \n",
    "            occupancy_grid[data>limit] = 1\n",
    "            occupancy_grid[data<=limit] = 0\n",
    "            occupancy_grid= occupancy_grid.transpose()\n",
    "    \n",
    "            # Displaying the map\n",
    "            occupancy_grid = occupancy_grid.astype(float)\n",
    "            ax.imshow(occupancy_grid.transpose(), cmap=cmap)\n",
    "            plt.title(\"Map : free cells in white, occupied cells in red\");\n",
    "            \n",
    "\n",
    "            # Run the A* algorithm\n",
    "             \n",
    "            #ici on fait le check infocam/filtre/pid, car la local nav est une boucle\n",
    "            start_x = robot_state.x\n",
    "            start_y = robot_state.y\n",
    "            start = truncate_coordinate(center_matrix_array,robot_state.x, robot_state.y)\n",
    "            \n",
    "            if goal == None:\n",
    "                goal = truncate_coordinate(center_matrix_array, x_end, y_end)\n",
    "                \n",
    "            x,y = np.mgrid[0:num_cases_x:1, 0:num_cases_y:1]\n",
    "            pos = np.empty(x.shape + (2,))\n",
    "            pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "            pos = np.reshape(pos, (x.shape[0]*x.shape[1], 2))\n",
    "            coords = list([(int(x[0]), int(x[1])) for x in pos])\n",
    "            h = np.linalg.norm(pos - goal, axis=-1)\n",
    "            h = dict(zip(coords, h))\n",
    "            print('A star started')\n",
    "            path, visitedNodes = A_Star(start, goal, h, coords, occupancy_grid, movement_type=\"4N\")\n",
    "            path = np.array(path).reshape(-1, 2).transpose()\n",
    "            visitedNodes = np.array(visitedNodes).reshape(-1, 2).transpose()\n",
    "\n",
    "            # Displaying the map\n",
    "            fig_astar, ax_astar = create_empty_plot(num_cases_x,num_cases_y)\n",
    "            ax_astar.imshow(occupancy_grid.transpose(), cmap=cmap)\n",
    "        \n",
    "            # Plot the best path found and the list of visited nodes\n",
    "            ax_astar.scatter(visitedNodes[0], visitedNodes[1], marker=\"o\", color = 'orange');\n",
    "            ax_astar.plot(path[0], path[1], marker=\"o\", color = 'blue');\n",
    "            ax_astar.scatter(start[0], start[1], marker=\"o\", color = 'green', s=200);\n",
    "            ax_astar.scatter(goal[0], goal[1], marker=\"o\", color = 'purple', s=200);\n",
    "            \n",
    "            # Displaying the map\n",
    "            fig_astar, ax_astar = create_empty_plot(num_cases_x,num_cases_y)\n",
    "            ax_astar.imshow(occupancy_grid.transpose(), cmap=cmap)\n",
    "                \n",
    "            # Plot the best path found and the list of visited nodes\n",
    "            ax_astar.scatter(visitedNodes[0], visitedNodes[1], marker=\"o\", color = 'orange');\n",
    "            ax_astar.plot(path[0], path[1], marker=\"o\", color = 'blue');\n",
    "            ax_astar.scatter(start[0], start[1], marker=\"o\", color = 'green', s=200);\n",
    "            ax_astar.scatter(goal[0], goal[1], marker=\"o\", color = 'purple', s=200);\n",
    "            \n",
    "            move_list = [0] * len(path[0])\n",
    "\n",
    "            for i in range(len(path[1])-1):\n",
    "                x1 = path[0][i]\n",
    "                x2 = path[0][i+1]\n",
    "                y1 = path[1][i]\n",
    "                y2 = path[1][i+1]\n",
    "                if (i+2<len(path[1])):\n",
    "                    x3 = path[0][i+2]\n",
    "                    y3 = path[1][i+2]\n",
    "                \n",
    "                _, center_matrix, _, _, robot_state, result_image = info_robot_cam()\n",
    "                \n",
    "                x_f, y_f, angle_f = ekf.get_pos()\n",
    "                correction(robot_state.x, robot_state.y, robot_state.angle, center_matrix_array[y1][x1][0], center_matrix_array[y1][x1][1], move_list[count], center_matrix_array[y2][x2][0], center_matrix_array[y2][x2][1])\n",
    "                rotate_robot(client)\n",
    "                _, center_matrix, _, _, robot_state, result_image = info_robot_cam()\n",
    "\n",
    "                for i in range(3):\n",
    "                    move_robot(client)\n",
    "                    aw(node.wait_for_variables({\"prox.horizontal\"}))\n",
    "                    obst = (list(node.v.prox.horizontal))\n",
    "                    obstacle_state = check_obstacle_state(obst)\n",
    "                    while obstacle_state == 0: \n",
    "                        \n",
    "                        for i in range(5):\n",
    "                            leftSpeed, rightSpeed = local_navi(obst)\n",
    "                            motor_set_speed(leftSpeed, rightSpeed, node)\n",
    "                            \n",
    "                        motor_set_speed(100,100,node)\n",
    "                        time.sleep(1.5)\n",
    "                        filter_call(100, 100, 1.5)\n",
    "                        motor_set_speed(0,0,node)\n",
    "                        obstacle_state = 2\n",
    "                        _, center_matrix, _, _, robot_state, result_image = info_robot_cam()\n",
    "                        x_f, y_f, angle_f = ekf.get_pos()\n",
    "                        correction(robot_state.x, robot_state.y, robot_state.angle, center_matrix_array[y3][x3][0], center_matrix_array[y3][x3][1], move_list[count],_, _)\n",
    "                        \n",
    "                        if obstacle_state == 2:\n",
    "                            print('obstacle')\n",
    "                            motor_set_speed(0, 0, node)\n",
    "                            break\n",
    "                    aw(node.wait_for_variables({\"prox.ground.delta\"}))\n",
    "                    ground_sensor = (list(node.v.prox.ground.delta))\n",
    "                    obstacle_state = check_kidnapped_state(ground_sensor, obstacle_state)\n",
    "                    while obstacle_state == 3:\n",
    "                        # kidnapped\n",
    "                        print(\"kidnapped\")\n",
    "                        v_stop = {\n",
    "                            \"motor.left.target\": [int(0)],\n",
    "                            \"motor.right.target\": [int(0)],\n",
    "                        }\n",
    "                        aw(node.set_variables(v_stop))\n",
    "                        aw(node.wait_for_variables({\"prox.ground.reflected\"}))\n",
    "                        ground_sensor = (list(node.v.prox.ground.reflected))\n",
    "                        obstacle_state = check_kidnapped_state(ground_sensor, state)\n",
    "                    if obstacle_state == 2:\n",
    "                        break\n",
    "                if obstacle_state == 2:\n",
    "                    obstacle_state = 1\n",
    "                    break\n",
    "        if abs(robot_state.x - center_matrix_array[goal[0]][goal[1]][0]) < 150 and abs(robot_state.y - center_matrix_array[goal[0]][goal[1]][1]) < 150 :\n",
    "            print(\"done\")\n",
    "            break\n",
    "                    \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57c9cf-4799-4469-b00b-ffbdb438beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_star_move(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce8925-1e24-4bfb-a88e-e85ab40b3903",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee5711-9472-4d15-83cb-6c3d5ec90404",
   "metadata": {},
   "source": [
    "In conclusion, this project highlighted the challenges of collaborative programming, where differing perspectives and interpretations can lead to difficulties. When bugs arose in the main algorithm, identifying and resolving them proved challenging due to the lack of comprehension of the algorithm in its entierity.\n",
    "\n",
    "\n",
    "Our experience with GitHub proved invaluable for project synchronization and streamlining algorithm transmission among team members. Moving forward, we recognize the importance of defining variables at the project's outset to avoid errors during the merge process. Additionally, establishing a standardized unit for distance, normalized during camera initialization, will enhance consistency.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d8fbe-fa13-418e-9a49-c09251cce032",
   "metadata": {},
   "source": [
    "### 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768ed61-8bb4-4e21-a666-02c63e372bff",
   "metadata": {},
   "source": [
    "ChatGPT to help with debugging\n",
    "\n",
    "filter: https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13897b0-bfeb-4331-89be-b6ebacb5d124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
