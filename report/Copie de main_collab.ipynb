{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQjH9qNvW_d-"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    \n",
    "<li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\" Introduction\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"#2.-Environment-of-the-project\" data-toc-modified-id=\"2.Environment of the project\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment of the project</a></span></li>\n",
    "    \n",
    "<li><span><a href=\"3.-Computer-vision\" data-toc-modified-id=\"3.-Computer-vision\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Computer vision</a></span>\n",
    "    <ul class=\"toc-item\"><li><span><a href=\"#Main-Goal\" data-toc-modified-id=\"Main-Goal-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Main Goal</a></span></li>\n",
    "    <li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Methodology</a></span></li></ul></li>\n",
    "\n",
    "<li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\" Introduction\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Introduction</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W53EXmwW_eC"
   },
   "source": [
    "# Vision guided navigation with Thymio - Final project\n",
    "Authors: Ahmed Boubakry, Charles de Fournas, Julie Favre, Paul Richard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCNt2lzOW_eD"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A54vLF53W_eE"
   },
   "source": [
    "The objective of this project is to develop a comprehensive robotic system for the Thymio robot, enabling it to navigate efficiently on a two-dimensional plane. The primary focus involves implementing advanced algorithms and methods to achieve global and local navigation, obstacle avoidance, accurate position estimation, and trajectory correction. The key components of our project include:\n",
    "\n",
    "- Computer Vision in order to create the map and retrieve the robot's position: Utilizing computer vision techniques, we aim to create a map of the environment and accurately determine the Thymio robot's position in real-time<br><br>\n",
    "\n",
    "\n",
    "- A* algorithm to find the optimal path:,Implementing the A* algorithm to find the optimal path from the starting position to the specified goal, considering the map and any obstacles encountered<br><br>\n",
    "\n",
    "- Local navigation to avoid physical objects: Developing local navigation methods to enable the Thymio robot to dynamically adjust its path in real-time, effectively avoiding physical obstacles encountered during its movement<br><br>\n",
    "\n",
    "\n",
    "- Filtering with Extended Kalman Filter to estimate accuretly the position: Employing the Extended Kalman Filter for precise and accurate estimation of the robot's position, enhancing the overall reliability of the navigation system and still poviding the position of the robot if we don't see the camera<br><br>\n",
    "\n",
    "- Controller to correct the trajectory after every move: The robot will change his direction in accord with the camera and theorical position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5912Xv_W_eE"
   },
   "source": [
    "## 2. Environment of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3qGxcsjW_eE"
   },
   "source": [
    "We have opted for a black and white grid as our environment, where white cells signify walkable areas and black cells represent walls. The robot's objective is to determine the optimal path from its starting position to a designated goal cell, indicated in blue.\n",
    "\n",
    "For visualization purposes, we print the grid on an A0 page with 8 columns and 6 rows. The page's borders serve as reference points for the camera to deduce the grid. We've outlined the grid using black tape, aiding in its detection.\n",
    "\n",
    "Obstacles are depicted by black cartoon representations, and the goal is visually distinguished by a blue cartoon. Additionally, on the robot, we've strategically positioned two black points – a smaller one in front of a larger one – to help the camera ascertain the robot's orientation.\n",
    "\n",
    "To enhance the camera's visibility, we've strategically placed multiple lamps to illuminate the area. These lamps are intended to provide ample lighting for improved camera performance.\n",
    "\n",
    "In consideration of the Thymio sensor, obstacles are designed to be solid, ensuring effective detection by the sensor. It's important to note that these obstacles are shaped differently from rounds, as our camera primarily detects round forms for our robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/env_2.jpg\" alt=\"environnement\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program begins with the initialization of global variables and the importation of modules. Initially, we extract the grid and the robot position. The subsequent significant part of our code focuses on finding the optimal path to reach the destination.\n",
    "\n",
    "Once the setup is complete, the main loop initiates. The robot moves from one position to another, and at regular intervals, the program analyzes and corrects its position. During each iteration, the loop checks for potential obstacles. If an obstacle is detected, a corresponding function is triggered to navigate around it. Otherwise, the program proceeds as usual, initiating a new loop to progress to the next step.\n",
    "\n",
    "This loop continues until the final destination is reached. The structure ensures that the robot dynamically adapts its path, corrects its trajectory, and navigates around obstacles, providing a robust and efficient navigation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/logic_map.jpg\" alt=\"Method overview\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai-Ep6buW_eF"
   },
   "source": [
    "## 3. Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgc8lb6lW_eF"
   },
   "source": [
    "code pour le cv et tout ce qu'on lit de la cam\n",
    "expliquer tout ce qu'on fait !\n",
    "ajouter image du décalage des centres\n",
    "ajouter image de cv (ou on voit robot et cadrage)\n",
    "\n",
    "We noticed that when the robot was on the edge of the map, the robot was shifted towards the center of the map, due to the angle with the camera. (see image) To correct this, we added a distortion correction array, that computes the virtual center of the cell from the camera point of vue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4fAi2rKW_eG"
   },
   "source": [
    "## 4. Path finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMcOqoRrW_eG"
   },
   "source": [
    "code pour A* move et tout ce qui est path finding related\n",
    "ajouter nos images plt ou on calcule le chemin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUJcQOQVW_eG"
   },
   "source": [
    "## 5. Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQjR4Rf_W_eH"
   },
   "source": [
    "vous avez compris le drill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db3hXbn9W_eH"
   },
   "source": [
    "## 6. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SETdLG4pW_eI"
   },
   "source": [
    "To be sure that we have the correct position of the robot even when the camera fails, we implemented a filtering feature. As our model was not linear, we went with the Extended Kalman Filter. This filter takes as input the position measured by the camera, and the instaneous speed of the robot. Every time it is called, it updates the position based on the previous position, the camera input, and the model of the robot (where it is supposed to be given the speed and previous position).\n",
    "\n",
    "We call the filter after every movement, giving the time period and the speed, thanks to the method `filter_call()` below.\n",
    "\n",
    "The rest of the code is all in ``ExtendedKalmanFilter.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kAlofzAW_eI"
   },
   "outputs": [],
   "source": [
    "ekf = EKF(1, initial_pos, initial_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npLi1qC2W_eJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLTcpnBjW_eJ"
   },
   "source": [
    "## 7. Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yPHdVRQW_eJ"
   },
   "source": [
    "As our robot advance and follow the path calculate by the global navigation, our robots deviate and we decide that a correction fonction is needed to assure the global trajectory. The first idea was to create a PID to follow the measured angle. But we couldn't apply the PID to our algorytme. That's why we create a second version to controle speciffecly the direction correction.\n",
    "\n",
    " After every cell the robot moved to, we check if its in the expected position. If the distance between the center of the robot and the center of the cell it is supposed to be in is too large, we execute the controller in two step. If the distance is good but the orientation of the robot is too different of the expected orientation, we execute the controller in one step. If it is good enough, we continue the path without interfering.\n",
    "The controller in three steps:\n",
    "- rotate the robot in direction of the center of the cell\n",
    "- move the robot towards the center\n",
    "- rotate the robot in the expected orientation to continue the path\n",
    "\n",
    "The controller in one step:\n",
    "- rotate the robot in the expected orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVHcFDPoW_eK"
   },
   "source": [
    "## 8. Project run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygSAZliAW_eK"
   },
   "source": [
    "main loop !\n",
    "blablater sur les choix de synchrone vs asynchrone et quelles methodes on appelle quand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG8X670wW_eK"
   },
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L5SExEWW_eL"
   },
   "source": [
    "In conclusion, we managed to implement a moving robot on the grid. The few drawbacks we have are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ezP58f2W_eL"
   },
   "source": [
    "### 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KPGhWTEW_eL"
   },
   "source": [
    "ChatGPT to help with debugging\n",
    "\n",
    "filter: https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFqVAcaRW_eL"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/julijul-3/MobileRobotics/blob/main/main_collab.ipynb",
     "timestamp": 1701892003584
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
